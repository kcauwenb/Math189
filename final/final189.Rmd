---
title: "final189"
author: "Kalyani Cauwenberghs"
date: "3/7/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Part 1
Instructions:
Compile a collection of tips for best data presentation, including the illustration and R script for each
 TODO: what does she mean for R script and illustration? resolved
 one should boxplot next to each other --> provide sample code how that's done
 use Rmd, show R code and plot.
#Part 2
Instructions:
For the final project, look back at all the analysis approaches you have used throughout the quarter. Consider HealthGen as the outcome, grouped into Excellent/Vgood, versus Good/Fair/Poor. (In the dataset dat.Rda, the HealthGen variable is assigned value 1 if its original value is Excellent/Vgood and is assigned value 0 otherwise.) Consider all other variables as potential predictors. Develop a comprehensive and reproducible analysis report, to explore the relationship between these variables and the outcome. Pay attention to (but not limited to) the following:

##1.)
Missing data: do not remove observations with any missing data from the start; after screening you might reduce to a smaller set of variables, therefore remove fewer observations at that point. Also you may consider removing variables with too much missing.



First step: remove the two columns that have missing data for more than half its observations
```{r}
load("dat (1).rda")
#remove columns with more than half the observations missing
dat<-dat[,-which(colSums(is.na(dat)) > nrow(dat)/2)]
```


Second step: for all categorical variables, reduce the number of categories by combining sparse categories
```{r echo = FALSE, message=FALSE, warning=FALSE}
#first combine categories:
#put mexican under hispanic
dat$Race1[which(dat$Race1=="Mexican")]<-"Hispanic"
dat$Race1<-factor(dat$Race1)
#split education into above or below high school grad
levels(dat$Education)<-c(levels(dat$Education), "HS_or_less", "college_or_more")
dat$Education[which(dat$Education=="8th Grade" |dat$Education=="9 - 11th Grade"|dat$Education=="High School")]<-"HS_or_less"
dat$Education[which(dat$Education=="Some College" |dat$Education=="College Grad")]<-"college_or_more"
dat$Education<-factor(dat$Education)
#split marital status into the following: (live partner, married), (divorced,separated, widowed), (never married)
levels(dat$MaritalStatus)<-c(levels(dat$MaritalStatus), "Together")
dat$MaritalStatus[c(which(dat$MaritalStatus=="LivePartner"), which(dat$MaritalStatus=="Married"))]<-"Together"
dat$MaritalStatus[c(which(dat$MaritalStatus=="Widowed"), which(dat$MaritalStatus=="Divorced"))]<-"Separated"
dat$MaritalStatus<-factor(dat$MaritalStatus)
#below mean vs above median income (65000)
levels(dat$HHIncome)<-c(levels(dat$HHIncome), "below_med", "above_med")
below_med<-levels(dat$HHIncome)[1:9]
above_med<-levels(dat$HHIncome)[10:12]
dat$HHIncome[which(dat$HHIncome %in% below_med)]<-"below_med"
dat$HHIncome[which(dat$HHIncome %in% above_med)]<-"above_med"
dat$HHIncome<-factor(dat$HHIncome)
#BMI: combine underweight with normal
levels(dat$BMI_WHO)<-c("12.0_to_24.9",levels(dat$BMI_WHO))
dat$BMI_WHO[c(which(dat$BMI_WHO == "12.0_18.5"),which(dat$BMI_WHO == "18.5_to_24.9"))]<-"12.0_to_24.9"
dat$BMI_WHO<-factor(dat$BMI_WHO)
#depressed: combine several with most
levels(dat$Depressed)<-c(levels(dat$Depressed), "Lots")
dat$Depressed[c(which(dat$Depressed=="Several"),which(dat$Depressed=="Most"))]<-"Lots"
dat$Depressed<-factor(dat$Depressed)
#comp hrs day: categories: (0,1) (2,+)
levels(dat$CompHrsDay)<-c(levels(dat$CompHrsDay), "one_or_less", "two_or_more")
one_or_less<-levels(dat$CompHrsDay)[1:3]
two_or_more<-levels(dat$CompHrsDay)[4:7]
dat$CompHrsDay[which(dat$CompHrsDay %in% one_or_less)]<-"one_or_less"
dat$CompHrsDay[which(dat$CompHrsDay %in% two_or_more)]<-"two_or_more"
dat$CompHrsDay<-factor(dat$CompHrsDay)
#TV hrs day: categories: (0,1) (2,+)
levels(dat$TVHrsDay)<-c(levels(dat$TVHrsDay), "one_or_less", "two_or_more")
dat$TVHrsDay[which(dat$TVHrsDay %in% one_or_less)]<-"one_or_less"
dat$TVHrsDay[which(dat$TVHrsDay %in% two_or_more)]<-"two_or_more"
dat$TVHrsDay<-factor(dat$TVHrsDay)
#sex orient: hetero vs other
levels(dat$SexOrientation)<-c(levels(dat$SexOrientation), "Other")
dat$SexOrientation[c(which(dat$SexOrientation=="Bisexual"),which(dat$SexOrientation=="Homosexual"))]<-"Other"
dat$SexOrientation<-factor(dat$SexOrientation)
```

```{r eval=FALSE}
for(i in 1:ncol(dat)){
  if(class(dat[,i])=="factor"){
    print(colnames(dat)[i])
    print(table(dat[,i])/length(dat[,i]))
  }
}
```



Third step: Univariately screen all the remaining predictors and get rid of those whose p-value > 0.2.
```{r echo = FALSE}
#puts the class of each column in df into an array
classify<-function(df) {
  array<-character(ncol(df))
  for (i in 1:ncol(df)){
    array[i]<-class(df[,i])
  }
  return(array)
}

#returns TRUE if ith column of dat passes the screening 
screen<-function(i) {
  var = dat[,i]
  no_miss<-which(!is.na(var)& !is.na(dat$HealthGen))
  p=1
  m<-glm(dat$HealthGen[no_miss]~var[no_miss], family = binomial())
  if(classify(dat)[i] == "integer"){
    p = coef(summary(m))[2,4]
  } else {
    null<-glm(dat$HealthGen[no_miss]~1, family = binomial())
    p=anova(null, m, test = "LRT")[2,5]
  }
  return(p<0.2)
}

#screen each column of dat excluding HealthGen. Generate a vector representing indices of variables we want to keep
var_indices<-which(colnames(dat) !="HealthGen")
for (i in var_indices) {
  if(!screen(i)) {
    var_indices<-var_indices[which(var_indices!=i)]
  }
}
print("our final screened variables:")
colnames(dat)[var_indices]
```


##2.)
Include “Table 1”
```{r echo = FALSE, eval=FALSE}
knitr::kable(mtcars[1:5,])
table1<-data.frame(numeric(10))
rownames(table1) <- colnames(dat)[var_indices]
```


##3.)
After univariate screening, building a multiple logistic regression model to predict the general health outcome of very good or excellent versus otherwise. State clearly your criteria at each step in the narrative.

General algorithm: TODO

At each step, we will keep track of the inidces of the variables
```{r echo = FALSE}
#first, remove all observations with any missing data
keep_rows<-which(rowSums(is.na(dat[,c(var_indices,8)]))==0)
dat<-dat[keep_rows,]

#given indices of predictors, returns a formula regressing HealthGen against the predictors to be put into glm
form<-function(ind){
  paste("HealthGen~", paste(colnames(dat)[ind], collapse = '+'), sep="")
}

BSS<-function(ind) {
  num_iter<-0
  AIC<-NULL
  R2<-NULL
  while(length(ind)>1){
    num_iter<-num_iter+1
    #start with full model
    m<-glm(form(ind), family=binomial(), data = dat)
    #compare it to all models that are like the full model, but with one variable removed
    #one p value for each temp model
    p<-numeric(length(ind))
    for(i in ind) {
      f<-form(ind[-which(ind==i)])
      if(length(ind)==1) {f<- "HealthGen~1"}
      m_temp<-glm(f, family=binomial(), data = dat)
      p[which(ind == i)]<-anova(m, m_temp, test = "LRT")[2,5]
    }
    cat("\np: ",p)
    if(sum(p>0.05)>0){
      ind<-ind[which(p<0.05)]
    } else {
      return(ind)
    }
    cat("\n\nind: ", ind)
  }
  return(ind)
}
#our final selected model
new_ind<-BSS(var_indices)
model<-glm(form(new_ind), family = binomial(), data = dat)
```


##4.)
Assess the predictability of the model by computing the (generalized) R-squared and the area under the ROC curve (AUC), as well as the cross-validated AUC.

Find generalized R-squared:
```{r echo = FALSE}
r2log<-function(glm_obj) {
  glm_null<-glm(HealthGen~1, family = binomial(), data = dat)
  #likelihood ratio test statistic
  t<-2*(logLik(glm_obj)-logLik(glm_null))/nobs(glm_obj)
  r2<-1-exp(-t)
  return(r2)
}
cat("##generalized R-squared: ", r2log(model))
```


ROC and AUC
```{r echo=FALSE, message=FALSE, warning=FALSE}
install.packages("pROC")
library(pROC)
install.packages("cvTools")
library(cvTools)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("Plotting ROC of our selected model")
plot.roc(dat$HealthGen, predict(model, type = "response"))
auc<-auc(dat$HealthGen, predict(model, type = "response"))
```


```{r echo = FALSE}
print("AUC of our selected model")
print(auc)
```

Cross-validated ROC and AUC

```{r echo=FALSE, message=FALSE, warning=FALSE}
folds <- cvFolds(n = nrow(dat), K = 10, R = 1)
auc = numeric(10)
for( i in 1:10){
  train = folds$subsets[folds$which != i]
  fit = glm(form(new_ind), family = binomial(), data = dat)
  auc[i]<-auc(dat[-train,]$HealthGen, predict(fit, newdata = dat[-train,], type = "response"))
}
```


```{r echo = FALSE}
print("final AUC:")
print(mean(auc))
```


##5.)
Use the variables that have passed the univariate screening, to build a classification tree. Describe clearly how you arrive at the final tree. Compute the error rate of your classification tree.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(rpart)
library(tree)
library(aplore3)
```


Approach 1:
```{r echo = FALSE}
#model
plot.new()
train=sample.int(nrow(dat),floor(nrow(dat)*0.8))
fit=rpart(form(new_ind), data = dat, subset=train)
plot(fit)
<<<<<<< HEAD
=======
a<- text(fit, use.n = TRUE)

```
```{r}


par(mfrow = c(1,2), xpd = NA)
plot.new()
text(fit, pretty=0)
plot(fit)
plot(fit)
>>>>>>> f675e9a1f8b804fa4bca58ee8458a83ecda0871b
text(fit, pretty=0, use.n = TRUE)
dev.off()
printcp(fit)
```
Approach 2:
```{r}

```



Testing error
```{r}

```


##6.)
Discuss any limitations in the analysis.


##Bonus
Explore random forest on the data above.
```{r}

```


